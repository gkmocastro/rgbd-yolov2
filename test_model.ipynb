{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import lightnet as ln\n",
    "from torchinfo import summary\n",
    "from model import model_builder\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ln.models.Darknet19(1000)\n",
    "# model.load('weights/darknet19_448.weights')\n",
    "\n",
    "# # Save as PyTorch weight file (Not strictly necessary, but it is faster than darknet weight files)\n",
    "# model.save('weights/darknet19_448.pt')\n",
    "\n",
    "# # Converting Darknet19 weights to Yolo (This is the same as the darknet19_448.conv.23.weights from darknet)\n",
    "# model.save('weights/yolo-pretrained_darknet.pt', remap=ln.models.YoloV2.remap_darknet19)\n",
    "\n",
    "# # Load yolo weights (Requires `strict=False`, because not all layers have weights in this file)\n",
    "# detection_model = ln.models.YoloV2(3)\n",
    "# detection_model.load('weights/yolo-pretrained_darknet.pt', strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Only Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "YoloV2 (YoloV2)                               [32, 3, 416, 416]    [32, 40, 13, 13]     --                   True\n",
       "├─FeatureExtractor (backbone)                 [32, 3, 416, 416]    [32, 1024, 13, 13]   --                   True\n",
       "│    └─Sequential (module)                    [32, 3, 416, 416]    [32, 1024, 13, 13]   --                   True\n",
       "│    │    └─Conv2dBatchAct (1_convbatch)      [32, 3, 416, 416]    [32, 32, 416, 416]   928                  True\n",
       "│    │    └─MaxPool2d (2_max)                 [32, 32, 416, 416]   [32, 32, 208, 208]   --                   --\n",
       "│    │    └─Conv2dBatchAct (3_convbatch)      [32, 32, 208, 208]   [32, 64, 208, 208]   18,560               True\n",
       "│    │    └─MaxPool2d (4_max)                 [32, 64, 208, 208]   [32, 64, 104, 104]   --                   --\n",
       "│    │    └─Conv2dBatchAct (5_convbatch)      [32, 64, 104, 104]   [32, 128, 104, 104]  73,984               True\n",
       "│    │    └─Conv2dBatchAct (6_convbatch)      [32, 128, 104, 104]  [32, 64, 104, 104]   8,320                True\n",
       "│    │    └─Conv2dBatchAct (7_convbatch)      [32, 64, 104, 104]   [32, 128, 104, 104]  73,984               True\n",
       "│    │    └─MaxPool2d (8_max)                 [32, 128, 104, 104]  [32, 128, 52, 52]    --                   --\n",
       "│    │    └─Conv2dBatchAct (9_convbatch)      [32, 128, 52, 52]    [32, 256, 52, 52]    295,424              True\n",
       "│    │    └─Conv2dBatchAct (10_convbatch)     [32, 256, 52, 52]    [32, 128, 52, 52]    33,024               True\n",
       "│    │    └─Conv2dBatchAct (11_convbatch)     [32, 128, 52, 52]    [32, 256, 52, 52]    295,424              True\n",
       "│    │    └─MaxPool2d (12_max)                [32, 256, 52, 52]    [32, 256, 26, 26]    --                   --\n",
       "│    │    └─Conv2dBatchAct (13_convbatch)     [32, 256, 26, 26]    [32, 512, 26, 26]    1,180,672            True\n",
       "│    │    └─Conv2dBatchAct (14_convbatch)     [32, 512, 26, 26]    [32, 256, 26, 26]    131,584              True\n",
       "│    │    └─Conv2dBatchAct (15_convbatch)     [32, 256, 26, 26]    [32, 512, 26, 26]    1,180,672            True\n",
       "│    │    └─Conv2dBatchAct (16_convbatch)     [32, 512, 26, 26]    [32, 256, 26, 26]    131,584              True\n",
       "│    │    └─Conv2dBatchAct (17_convbatch)     [32, 256, 26, 26]    [32, 512, 26, 26]    1,180,672            True\n",
       "│    │    └─MaxPool2d (18_max)                [32, 512, 26, 26]    [32, 512, 13, 13]    --                   --\n",
       "│    │    └─Conv2dBatchAct (19_convbatch)     [32, 512, 13, 13]    [32, 1024, 13, 13]   4,720,640            True\n",
       "│    │    └─Conv2dBatchAct (20_convbatch)     [32, 1024, 13, 13]   [32, 512, 13, 13]    525,312              True\n",
       "│    │    └─Conv2dBatchAct (21_convbatch)     [32, 512, 13, 13]    [32, 1024, 13, 13]   4,720,640            True\n",
       "│    │    └─Conv2dBatchAct (22_convbatch)     [32, 1024, 13, 13]   [32, 512, 13, 13]    525,312              True\n",
       "│    │    └─Conv2dBatchAct (23_convbatch)     [32, 512, 13, 13]    [32, 1024, 13, 13]   4,720,640            True\n",
       "├─ModuleList (neck)                           --                   --                   --                   True\n",
       "│    └─Sequential (0)                         [32, 1024, 13, 13]   [32, 1024, 13, 13]   --                   True\n",
       "│    │    └─Conv2dBatchAct (0)                [32, 1024, 13, 13]   [32, 1024, 13, 13]   9,439,232            True\n",
       "│    │    └─Conv2dBatchAct (1)                [32, 1024, 13, 13]   [32, 1024, 13, 13]   9,439,232            True\n",
       "│    └─Sequential (1)                         [32, 512, 26, 26]    [32, 256, 13, 13]    --                   True\n",
       "│    │    └─Conv2dBatchAct (0)                [32, 512, 26, 26]    [32, 64, 26, 26]     32,896               True\n",
       "│    │    └─Reorg (1)                         [32, 64, 26, 26]     [32, 256, 13, 13]    --                   --\n",
       "├─Sequential (head)                           [32, 1280, 13, 13]   [32, 40, 13, 13]     --                   True\n",
       "│    └─Conv2dBatchAct (0)                     [32, 1280, 13, 13]   [32, 1024, 13, 13]   --                   True\n",
       "│    │    └─ModuleList (layers)               --                   --                   11,798,528           True\n",
       "│    └─Conv2d (1)                             [32, 1024, 13, 13]   [32, 40, 13, 13]     41,000               True\n",
       "=============================================================================================================================\n",
       "Total params: 50,568,264\n",
       "Trainable params: 50,568,264\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 469.30\n",
       "=============================================================================================================================\n",
       "Input size (MB): 66.45\n",
       "Forward/backward pass size (MB): 8264.12\n",
       "Params size (MB): 202.27\n",
       "Estimated Total Size (MB): 8532.84\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_model = model_builder(num_classes=3, model_type=\"rgb\")\n",
    "summary(model=rgb_model, \n",
    "        input_size=(32, 3, 416, 416), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing first layer with new conv: Conv2dBatchAct(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), LeakyReLU(negative_slope=0.1, inplace=True))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "YoloV2 (YoloV2)                               [32, 1, 416, 416]    [32, 40, 13, 13]     --                   True\n",
       "├─FeatureExtractor (backbone)                 [32, 1, 416, 416]    [32, 1024, 13, 13]   --                   True\n",
       "│    └─Sequential (module)                    [32, 1, 416, 416]    [32, 1024, 13, 13]   --                   True\n",
       "│    │    └─Conv2dBatchAct (1_convbatch)      [32, 1, 416, 416]    [32, 32, 416, 416]   352                  True\n",
       "│    │    └─MaxPool2d (2_max)                 [32, 32, 416, 416]   [32, 32, 208, 208]   --                   --\n",
       "│    │    └─Conv2dBatchAct (3_convbatch)      [32, 32, 208, 208]   [32, 64, 208, 208]   18,560               True\n",
       "│    │    └─MaxPool2d (4_max)                 [32, 64, 208, 208]   [32, 64, 104, 104]   --                   --\n",
       "│    │    └─Conv2dBatchAct (5_convbatch)      [32, 64, 104, 104]   [32, 128, 104, 104]  73,984               True\n",
       "│    │    └─Conv2dBatchAct (6_convbatch)      [32, 128, 104, 104]  [32, 64, 104, 104]   8,320                True\n",
       "│    │    └─Conv2dBatchAct (7_convbatch)      [32, 64, 104, 104]   [32, 128, 104, 104]  73,984               True\n",
       "│    │    └─MaxPool2d (8_max)                 [32, 128, 104, 104]  [32, 128, 52, 52]    --                   --\n",
       "│    │    └─Conv2dBatchAct (9_convbatch)      [32, 128, 52, 52]    [32, 256, 52, 52]    295,424              True\n",
       "│    │    └─Conv2dBatchAct (10_convbatch)     [32, 256, 52, 52]    [32, 128, 52, 52]    33,024               True\n",
       "│    │    └─Conv2dBatchAct (11_convbatch)     [32, 128, 52, 52]    [32, 256, 52, 52]    295,424              True\n",
       "│    │    └─MaxPool2d (12_max)                [32, 256, 52, 52]    [32, 256, 26, 26]    --                   --\n",
       "│    │    └─Conv2dBatchAct (13_convbatch)     [32, 256, 26, 26]    [32, 512, 26, 26]    1,180,672            True\n",
       "│    │    └─Conv2dBatchAct (14_convbatch)     [32, 512, 26, 26]    [32, 256, 26, 26]    131,584              True\n",
       "│    │    └─Conv2dBatchAct (15_convbatch)     [32, 256, 26, 26]    [32, 512, 26, 26]    1,180,672            True\n",
       "│    │    └─Conv2dBatchAct (16_convbatch)     [32, 512, 26, 26]    [32, 256, 26, 26]    131,584              True\n",
       "│    │    └─Conv2dBatchAct (17_convbatch)     [32, 256, 26, 26]    [32, 512, 26, 26]    1,180,672            True\n",
       "│    │    └─MaxPool2d (18_max)                [32, 512, 26, 26]    [32, 512, 13, 13]    --                   --\n",
       "│    │    └─Conv2dBatchAct (19_convbatch)     [32, 512, 13, 13]    [32, 1024, 13, 13]   4,720,640            True\n",
       "│    │    └─Conv2dBatchAct (20_convbatch)     [32, 1024, 13, 13]   [32, 512, 13, 13]    525,312              True\n",
       "│    │    └─Conv2dBatchAct (21_convbatch)     [32, 512, 13, 13]    [32, 1024, 13, 13]   4,720,640            True\n",
       "│    │    └─Conv2dBatchAct (22_convbatch)     [32, 1024, 13, 13]   [32, 512, 13, 13]    525,312              True\n",
       "│    │    └─Conv2dBatchAct (23_convbatch)     [32, 512, 13, 13]    [32, 1024, 13, 13]   4,720,640            True\n",
       "├─ModuleList (neck)                           --                   --                   --                   True\n",
       "│    └─Sequential (0)                         [32, 1024, 13, 13]   [32, 1024, 13, 13]   --                   True\n",
       "│    │    └─Conv2dBatchAct (0)                [32, 1024, 13, 13]   [32, 1024, 13, 13]   9,439,232            True\n",
       "│    │    └─Conv2dBatchAct (1)                [32, 1024, 13, 13]   [32, 1024, 13, 13]   9,439,232            True\n",
       "│    └─Sequential (1)                         [32, 512, 26, 26]    [32, 256, 13, 13]    --                   True\n",
       "│    │    └─Conv2dBatchAct (0)                [32, 512, 26, 26]    [32, 64, 26, 26]     32,896               True\n",
       "│    │    └─Reorg (1)                         [32, 64, 26, 26]     [32, 256, 13, 13]    --                   --\n",
       "├─Sequential (head)                           [32, 1280, 13, 13]   [32, 40, 13, 13]     --                   True\n",
       "│    └─Conv2dBatchAct (0)                     [32, 1280, 13, 13]   [32, 1024, 13, 13]   --                   True\n",
       "│    │    └─ModuleList (layers)               --                   --                   11,798,528           True\n",
       "│    └─Conv2d (1)                             [32, 1024, 13, 13]   [32, 40, 13, 13]     41,000               True\n",
       "=============================================================================================================================\n",
       "Total params: 50,567,688\n",
       "Trainable params: 50,567,688\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 466.11\n",
       "=============================================================================================================================\n",
       "Input size (MB): 22.15\n",
       "Forward/backward pass size (MB): 8264.12\n",
       "Params size (MB): 202.27\n",
       "Estimated Total Size (MB): 8488.54\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_model = model_builder(num_classes=3, model_type=\"depth\")\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=depth_model, \n",
    "        input_size=(32, 1, 416, 416), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13368/3341212066.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  depth_pretrained = torch.load(\"models/depth_99.pth\")\n"
     ]
    }
   ],
   "source": [
    "depth_pretrained = torch.load(\"models/depth_99.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backbone.module.1_convbatch.layers.0.weight',\n",
       " 'backbone.module.1_convbatch.layers.1.weight',\n",
       " 'backbone.module.1_convbatch.layers.1.bias',\n",
       " 'backbone.module.1_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.1_convbatch.layers.1.running_var',\n",
       " 'backbone.module.1_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.3_convbatch.layers.0.weight',\n",
       " 'backbone.module.3_convbatch.layers.1.weight',\n",
       " 'backbone.module.3_convbatch.layers.1.bias',\n",
       " 'backbone.module.3_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.3_convbatch.layers.1.running_var',\n",
       " 'backbone.module.3_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.5_convbatch.layers.0.weight',\n",
       " 'backbone.module.5_convbatch.layers.1.weight',\n",
       " 'backbone.module.5_convbatch.layers.1.bias',\n",
       " 'backbone.module.5_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.5_convbatch.layers.1.running_var',\n",
       " 'backbone.module.5_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.6_convbatch.layers.0.weight',\n",
       " 'backbone.module.6_convbatch.layers.1.weight',\n",
       " 'backbone.module.6_convbatch.layers.1.bias',\n",
       " 'backbone.module.6_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.6_convbatch.layers.1.running_var',\n",
       " 'backbone.module.6_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.7_convbatch.layers.0.weight',\n",
       " 'backbone.module.7_convbatch.layers.1.weight',\n",
       " 'backbone.module.7_convbatch.layers.1.bias',\n",
       " 'backbone.module.7_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.7_convbatch.layers.1.running_var',\n",
       " 'backbone.module.7_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.9_convbatch.layers.0.weight',\n",
       " 'backbone.module.9_convbatch.layers.1.weight',\n",
       " 'backbone.module.9_convbatch.layers.1.bias',\n",
       " 'backbone.module.9_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.9_convbatch.layers.1.running_var',\n",
       " 'backbone.module.9_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.10_convbatch.layers.0.weight',\n",
       " 'backbone.module.10_convbatch.layers.1.weight',\n",
       " 'backbone.module.10_convbatch.layers.1.bias',\n",
       " 'backbone.module.10_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.10_convbatch.layers.1.running_var',\n",
       " 'backbone.module.10_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.11_convbatch.layers.0.weight',\n",
       " 'backbone.module.11_convbatch.layers.1.weight',\n",
       " 'backbone.module.11_convbatch.layers.1.bias',\n",
       " 'backbone.module.11_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.11_convbatch.layers.1.running_var',\n",
       " 'backbone.module.11_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.13_convbatch.layers.0.weight',\n",
       " 'backbone.module.13_convbatch.layers.1.weight',\n",
       " 'backbone.module.13_convbatch.layers.1.bias',\n",
       " 'backbone.module.13_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.13_convbatch.layers.1.running_var',\n",
       " 'backbone.module.13_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.14_convbatch.layers.0.weight',\n",
       " 'backbone.module.14_convbatch.layers.1.weight',\n",
       " 'backbone.module.14_convbatch.layers.1.bias',\n",
       " 'backbone.module.14_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.14_convbatch.layers.1.running_var',\n",
       " 'backbone.module.14_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.15_convbatch.layers.0.weight',\n",
       " 'backbone.module.15_convbatch.layers.1.weight',\n",
       " 'backbone.module.15_convbatch.layers.1.bias',\n",
       " 'backbone.module.15_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.15_convbatch.layers.1.running_var',\n",
       " 'backbone.module.15_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.16_convbatch.layers.0.weight',\n",
       " 'backbone.module.16_convbatch.layers.1.weight',\n",
       " 'backbone.module.16_convbatch.layers.1.bias',\n",
       " 'backbone.module.16_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.16_convbatch.layers.1.running_var',\n",
       " 'backbone.module.16_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.17_convbatch.layers.0.weight',\n",
       " 'backbone.module.17_convbatch.layers.1.weight',\n",
       " 'backbone.module.17_convbatch.layers.1.bias',\n",
       " 'backbone.module.17_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.17_convbatch.layers.1.running_var',\n",
       " 'backbone.module.17_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.19_convbatch.layers.0.weight',\n",
       " 'backbone.module.19_convbatch.layers.1.weight',\n",
       " 'backbone.module.19_convbatch.layers.1.bias',\n",
       " 'backbone.module.19_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.19_convbatch.layers.1.running_var',\n",
       " 'backbone.module.19_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.20_convbatch.layers.0.weight',\n",
       " 'backbone.module.20_convbatch.layers.1.weight',\n",
       " 'backbone.module.20_convbatch.layers.1.bias',\n",
       " 'backbone.module.20_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.20_convbatch.layers.1.running_var',\n",
       " 'backbone.module.20_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.21_convbatch.layers.0.weight',\n",
       " 'backbone.module.21_convbatch.layers.1.weight',\n",
       " 'backbone.module.21_convbatch.layers.1.bias',\n",
       " 'backbone.module.21_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.21_convbatch.layers.1.running_var',\n",
       " 'backbone.module.21_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.22_convbatch.layers.0.weight',\n",
       " 'backbone.module.22_convbatch.layers.1.weight',\n",
       " 'backbone.module.22_convbatch.layers.1.bias',\n",
       " 'backbone.module.22_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.22_convbatch.layers.1.running_var',\n",
       " 'backbone.module.22_convbatch.layers.1.num_batches_tracked',\n",
       " 'backbone.module.23_convbatch.layers.0.weight',\n",
       " 'backbone.module.23_convbatch.layers.1.weight',\n",
       " 'backbone.module.23_convbatch.layers.1.bias',\n",
       " 'backbone.module.23_convbatch.layers.1.running_mean',\n",
       " 'backbone.module.23_convbatch.layers.1.running_var',\n",
       " 'backbone.module.23_convbatch.layers.1.num_batches_tracked',\n",
       " 'neck.0.0.layers.0.weight',\n",
       " 'neck.0.0.layers.1.weight',\n",
       " 'neck.0.0.layers.1.bias',\n",
       " 'neck.0.0.layers.1.running_mean',\n",
       " 'neck.0.0.layers.1.running_var',\n",
       " 'neck.0.0.layers.1.num_batches_tracked',\n",
       " 'neck.0.1.layers.0.weight',\n",
       " 'neck.0.1.layers.1.weight',\n",
       " 'neck.0.1.layers.1.bias',\n",
       " 'neck.0.1.layers.1.running_mean',\n",
       " 'neck.0.1.layers.1.running_var',\n",
       " 'neck.0.1.layers.1.num_batches_tracked',\n",
       " 'neck.1.0.layers.0.weight',\n",
       " 'neck.1.0.layers.1.weight',\n",
       " 'neck.1.0.layers.1.bias',\n",
       " 'neck.1.0.layers.1.running_mean',\n",
       " 'neck.1.0.layers.1.running_var',\n",
       " 'neck.1.0.layers.1.num_batches_tracked',\n",
       " 'head.0.layers.0.weight',\n",
       " 'head.0.layers.1.weight',\n",
       " 'head.0.layers.1.bias',\n",
       " 'head.0.layers.1.running_mean',\n",
       " 'head.0.layers.1.running_var',\n",
       " 'head.0.layers.1.num_batches_tracked',\n",
       " 'head.1.weight',\n",
       " 'head.1.bias',\n",
       " '_LN_MODEL_VERSION']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(depth_pretrained.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rgb_state_dict = torch.load(\"models/rgb_state_dict.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights equivalent\n"
     ]
    }
   ],
   "source": [
    "fusion_model = model_builder(num_classes=3, model_type=\"rgbd\", fuse_layer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2dBatchAct: 3, Conv2d: 5, BatchNorm2d: 5, LeakyReLU: 5, Conv2dBatchAct: 3, Conv2d: 5, BatchNorm2d: 5, LeakyReLU: 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/models/_network_yolo_fusion.py:189\u001b[0m, in \u001b[0;36mYoloFusion.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Sequence 1\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Passthrough\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/layer/_fusion.py:211\u001b[0m, in \u001b[0;36mFusionSequential.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of input channels should be divisible by 2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 211\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregular\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion(x[:, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 676.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 168.31 MiB is free. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.42 GiB is allocated by PyTorch, and 34.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print a summary using torchinfo (uncomment for actual output)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# make sure this is \"input_size\", not \"input_shape\"\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# col_names=[\"input_size\"], # uncomment for smaller output\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvar_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2dBatchAct: 3, Conv2d: 5, BatchNorm2d: 5, LeakyReLU: 5, Conv2dBatchAct: 3, Conv2d: 5, BatchNorm2d: 5, LeakyReLU: 5]"
     ]
    }
   ],
   "source": [
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=fusion_model, \n",
    "        input_size=(32, 4, 416, 416), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(32, 4, 416, 416)\n",
    "output_tensor = fusion_model(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_weight = fusion_model.state_dict()['layers.1.regular.11_convbatch.layers.1.weight'].cpu()\n",
    "rgb_weight = rgb_model.state_dict()['backbone.module.11_convbatch.layers.1.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(fusion_weight, rgb_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
