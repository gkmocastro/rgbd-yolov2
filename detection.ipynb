{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv2 Output Cell Shape Explanation\n",
    "\n",
    "The output cell shape of the YOLOv2 architecture refers to the structure of the final feature map produced by the network, which is used for object detection. To understand this, letâ€™s break it down step-by-step:\n",
    "\n",
    "1. **Grid Division**  \n",
    "YOLOv2 divides the input image into a grid of size S x S. Each cell in this grid is responsible for predicting bounding boxes for objects whose centers fall within that cell. For instance, if the input image size is 416 x 416, a common grid size in YOLOv2 is 13 x 13, meaning each grid cell covers 32 x 32 pixels of the original image.\n",
    "\n",
    "\n",
    "2. **Bounding Box Predictions**  \n",
    "Each grid cell predicts:\n",
    "\n",
    "A fixed number of bounding boxes (typically 5).\n",
    "For each bounding box, it predicts:\n",
    "The coordinates: (x, y, w, h) representing the center coordinates of the box relative to the cell, as well as its width and height.\n",
    "The confidence score that indicates the likelihood that the box contains an object.\n",
    "\n",
    "\n",
    "3. **Class Predictions**  \n",
    "For each bounding box, YOLOv2 also predicts the probabilities that the object belongs to one of the predefined classes. If there are C classes, then for each bounding box, there are C class scores.\n",
    "\n",
    "\n",
    "4. **Output Tensor Shape**  \n",
    "The final output of YOLOv2 has the shape: S x S x (B x (5 + C))\n",
    "Where:\n",
    "\n",
    "S x S is the grid size (e.g., 13 x 13).\n",
    "B is the number of bounding boxes predicted per grid cell (typically 5).\n",
    "5 + C refers to the 5 values for each bounding box (4 for x, y, w, h and 1 for the confidence score) plus the class predictions C.\n",
    "Example:\n",
    "For a 13 x 13 grid, 5 bounding boxes per cell, and 20 classes (like in the Pascal VOC dataset):\n",
    "\n",
    "The output shape would be 13 x 13 x (5 x (5 + 20)) = 13 x 13 x 125.\n",
    "Each cell in this final output represents predictions for multiple bounding boxes and the associated class probabilities.\n",
    "\n",
    "This grid of predictions is then post-processed using techniques like non-maximum suppression (NMS) to filter out overlapping and low-confidence boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkmo/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import model_builder\n",
    "import torch\n",
    "import lightnet as ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkmo/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model shape\n",
    "\n",
    "def test(model):\n",
    "    X = torch.randn((2, 3, 416, 416))\n",
    "    print(model(X).shape)\n",
    "\n",
    "def test_loss(model, loss_fn):\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    X = torch.rand((1, 3, 416, 416))\n",
    "    y = torch.rand((1, 5, 5))\n",
    "    print(loss_fn)\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model(X)\n",
    "\n",
    "    print(y.shape)\n",
    "    print(y_pred.shape)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 40, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# S x S x (B x (5 + C)) -> (BATCH_SIZE, 5*(5+C), 13, 13)\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test framework loss\n",
    "\n",
    "loss_fn = ln.network.loss.RegionLoss(\n",
    "    num_classes= model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegionLoss(\n",
      "  classes=3, network_stride=32, IoU threshold=0.6, seen=2\n",
      "  coord_scale=1.0, object_scale=5.0, noobject_scale=1.0, class_scale=1.0\n",
      "  anchors=[1.3221, 1.7314] [3.1927, 4.0094] [5.0559, 8.0989] [9.4711, 4.8405] [11.236, 10.007]\n",
      ")\n",
      "torch.Size([1, 2, 5])\n",
      "torch.Size([1, 30, 13, 13])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3]' is invalid for input of size 845",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/lightnet/network/loss/_base.py:29\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 29\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduction:\n\u001b[1;32m     31\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(ret)\n",
      "File \u001b[0;32m~/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/lightnet/network/loss/_region.py:164\u001b[0m, in \u001b[0;36mRegionLoss.forward\u001b[0;34m(self, output, target, seen)\u001b[0m\n\u001b[1;32m    162\u001b[0m conf \u001b[38;5;241m=\u001b[39m output[:, :, \u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nC \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Create prediction boxes\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(nB\u001b[38;5;241m*\u001b[39mnA\u001b[38;5;241m*\u001b[39mnPixels, \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3]' is invalid for input of size 845"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "X = torch.rand((1, 3, 416, 416))\n",
    "y = torch.rand((1, 2, 5))\n",
    "print(loss_fn)\n",
    "y_pred = model(X)\n",
    "\n",
    "print(y.shape)\n",
    "print(y_pred.shape)\n",
    "loss = loss_fn(y_pred, y)\n",
    "print(loss.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegionLoss(\n",
      "  classes=1, network_stride=32, IoU threshold=0.6, seen=0\n",
      "  coord_scale=1.0, object_scale=5.0, noobject_scale=1.0, class_scale=1.0\n",
      "  anchors=[1.3221, 1.7314] [3.1927, 4.0094] [5.0559, 8.0989] [9.4711, 4.8405] [11.236, 10.007]\n",
      ")\n",
      "{'total': tensor(120.5731), 'conf': tensor(113.4837), 'coord': tensor(7.0893), 'class': tensor(0.)}\n"
     ]
    }
   ],
   "source": [
    "model = ln.models.YoloV2(1)\n",
    "\n",
    "# Create accompanying loss (minimal required arguments for it to work with our defined Yolo network)\n",
    "loss = ln.network.loss.RegionLoss(\n",
    "    num_classes=model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n",
    "print(loss)\n",
    "\n",
    "# Use loss\n",
    "input_tensor = torch.rand(1, 3, 416, 416)   # batch, channel, height, width\n",
    "target_tensor = torch.rand(1, 2, 5)         # batch, num_anno, 5 (see RegionLoss docs)\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "loss_value = loss(output_tensor, target_tensor)\n",
    "loss_value.backward()\n",
    "\n",
    "# Print loss\n",
    "print(loss.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
