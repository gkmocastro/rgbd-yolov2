{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv2 Output Cell Shape Explanation\n",
    "\n",
    "The output cell shape of the YOLOv2 architecture refers to the structure of the final feature map produced by the network, which is used for object detection. To understand this, let’s break it down step-by-step:\n",
    "\n",
    "1. **Grid Division**  \n",
    "YOLOv2 divides the input image into a grid of size S x S. Each cell in this grid is responsible for predicting bounding boxes for objects whose centers fall within that cell. For instance, if the input image size is 416 x 416, a common grid size in YOLOv2 is 13 x 13, meaning each grid cell covers 32 x 32 pixels of the original image.\n",
    "\n",
    "\n",
    "2. **Bounding Box Predictions**  \n",
    "Each grid cell predicts:\n",
    "\n",
    "A fixed number of bounding boxes (typically 5).\n",
    "For each bounding box, it predicts:\n",
    "The coordinates: (x, y, w, h) representing the center coordinates of the box relative to the cell, as well as its width and height.\n",
    "The confidence score that indicates the likelihood that the box contains an object.\n",
    "\n",
    "\n",
    "3. **Class Predictions**  \n",
    "For each bounding box, YOLOv2 also predicts the probabilities that the object belongs to one of the predefined classes. If there are C classes, then for each bounding box, there are C class scores.\n",
    "\n",
    "\n",
    "4. **Output Tensor Shape**  \n",
    "The final output of YOLOv2 has the shape: S x S x (B x (5 + C))\n",
    "Where:\n",
    "\n",
    "S x S is the grid size (e.g., 13 x 13).\n",
    "B is the number of bounding boxes predicted per grid cell (typically 5).\n",
    "5 + C refers to the 5 values for each bounding box (4 for x, y, w, h and 1 for the confidence score) plus the class predictions C.\n",
    "Example:\n",
    "For a 13 x 13 grid, 5 bounding boxes per cell, and 20 classes (like in the Pascal VOC dataset):\n",
    "\n",
    "The output shape would be 13 x 13 x (5 x (5 + 20)) = 13 x 13 x 125.\n",
    "Each cell in this final output represents predictions for multiple bounding boxes and the associated class probabilities.\n",
    "\n",
    "This grid of predictions is then post-processed using techniques like non-maximum suppression (NMS) to filter out overlapping and low-confidence boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import model_builder\n",
    "import torch\n",
    "import lightnet as ln\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading darknet weights (download: http://pjreddie.com/media/files/darknet19_448.weights)\n",
    "# model = ln.models.Darknet19(1000)\n",
    "# model.load('weights/darknet19_448.weights')\n",
    "\n",
    "# # Save as PyTorch weight file (Not strictly necessary, but it is faster than darknet weight files)\n",
    "# model.save('weights/darknet19_448.pt')\n",
    "\n",
    "# # Converting Darknet19 weights to Yolo (This is the same as the darknet19_448.conv.23.weights from darknet)\n",
    "# model.save('weights/yolo-pretrained_darknet.pt', remap=ln.models.YoloV2.remap_darknet19)\n",
    "\n",
    "# # Load yolo weights (Requires `strict=False`, because not all layers have weights in this file)\n",
    "# detection_model = ln.models.YoloV2(20)\n",
    "# detection_model.load('weights/yolo-pretrained_darknet.pt', strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 40, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "## Test model shape\n",
    "\n",
    "X = torch.randn((16, 3, 416, 416))\n",
    "print(model(X).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test framework loss\n",
    "\n",
    "loss_fn = ln.network.loss.RegionLoss(\n",
    "    num_classes= model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegionLoss(\n",
      "  classes=3, network_stride=32, IoU threshold=0.6, seen=0\n",
      "  coord_scale=1.0, object_scale=5.0, noobject_scale=1.0, class_scale=1.0\n",
      "  anchors=[1.3221, 1.7314] [3.1927, 4.0094] [5.0559, 8.0989] [9.4711, 4.8405] [11.236, 10.007]\n",
      ")\n",
      "{'total': tensor(252.8157), 'conf': tensor(158.1815), 'coord': tensor(64.0366), 'class': tensor(30.5976)}\n",
      "252.81565856933594\n"
     ]
    }
   ],
   "source": [
    "# Create accompanying loss (minimal required arguments for it to work with our defined Yolo network)\n",
    "loss = ln.network.loss.RegionLoss(\n",
    "    num_classes=model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n",
    "print(loss)\n",
    "\n",
    "# Use loss\n",
    "input_tensor = torch.rand(1, 3, 416, 416)   # batch, channel, height, width\n",
    "target_tensor = torch.rand(1, 22, 5)         # batch, num_anno, 5 (see RegionLoss docs)\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "loss_value = loss(output_tensor, target_tensor)\n",
    "#loss_value.backward()\n",
    "\n",
    "# Print loss\n",
    "print(loss.values)\n",
    "print(loss.values[\"total\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.1571044921875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.values[\"total\"].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path(\"data\")\n",
    "labels_dir = root_dir / \"labels\"\n",
    "img_dir = root_dir / \"data_object_image_2/training/image_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import YoloDarknetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kitti_dataset = YoloDarknetDataset(images_dir=img_dir, labels_dir=labels_dir, classes=[\"Cyclist\", \"Pedestrian\", \"car\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Kitti_dataset[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor = sample[1][\"boxes\"]\n",
    "target_tensor = target_tensor.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 13, 13])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': tensor(223.0321), 'conf': tensor(126.0392), 'coord': tensor(77.2070), 'class': tensor(19.7859)}\n"
     ]
    }
   ],
   "source": [
    "loss_value = loss(output_tensor, target_tensor)\n",
    "\n",
    "#loss_value.backward()\n",
    "\n",
    "# Print loss\n",
    "print(loss.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of labels in a file is: ('004139.txt', 22)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def max_labels_in_folder(folder_path):\n",
    "    max_labels = 0\n",
    "    max_labels_file = \"\"\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                label_count = sum(1 for line in file)  # Count lines in the file\n",
    "                if label_count > max_labels:\n",
    "                    max_labels = label_count\n",
    "                    max_labels_file = filename  # Update filename with the maximum labels\n",
    "    \n",
    "    return max_labels_file, max_labels\n",
    "\n",
    "# Set the path to your folder with the txt files\n",
    "folder_path = '/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/data/labels'\n",
    "max_labels = max_labels_in_folder(folder_path)\n",
    "print(f\"The maximum number of labels in a file is: {max_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 185.4051\n",
      "Epoch [2/5], Loss: 215.8996\n",
      "Epoch [3/5], Loss: 165.4923\n",
      "Epoch [4/5], Loss: 141.7955\n",
      "Epoch [5/5], Loss: 109.8457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[185.40510995047433,\n",
       " 215.8995840890067,\n",
       " 165.49234662737166,\n",
       " 141.79553876604353,\n",
       " 109.84571838378906]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import YoloDarknetDataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from train import train_yolov2\n",
    "import os\n",
    "from model import model_builder\n",
    "import lightnet as ln\n",
    "\n",
    "IMG_DIR = \"/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/data/images_test/\"\n",
    "LABEL_DIR =  \"/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/data/labels\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using Device {DEVICE}\")\n",
    "\n",
    "model = model_builder(num_classes=3)\n",
    "\n",
    "loss_fn = ln.network.loss.RegionLoss(\n",
    "    num_classes= model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = YoloDarknetDataset(\n",
    "    images_dir=IMG_DIR,\n",
    "    labels_dir=LABEL_DIR,\n",
    "    classes=[\"Cyclist\", \"Pedestrian\", \"car\"],\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "train_yolov2(model=model, \n",
    "             train_dataloader=train_dataloader, \n",
    "             loss_fn=loss_fn, \n",
    "             optimizer=optimizer, \n",
    "             num_epochs=NUM_EPOCHS, \n",
    "             device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000,  0.1620,  0.7552,  0.3239,  0.4843],\n",
       "        [ 2.0000,  0.3862,  0.7346,  0.2332,  0.5149],\n",
       "        [ 2.0000,  0.8769,  0.7619,  0.2445,  0.4710],\n",
       "        [ 2.0000,  0.5308,  0.5831,  0.0993,  0.2266],\n",
       "        [ 2.0000,  0.6173,  0.5030,  0.0411,  0.1056],\n",
       "        [ 2.0000,  0.7411,  0.5580,  0.0579,  0.1650],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[8][1][\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m targets \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m8\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/loss/_base.py:29\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 29\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduction:\n\u001b[1;32m     31\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(ret)\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/loss/_region.py:189\u001b[0m, in \u001b[0;36mRegionLoss.forward\u001b[0;34m(self, output, target, seen)\u001b[0m\n\u001b[1;32m    186\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Get target values\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m coord_mask \u001b[38;5;241m=\u001b[39m coord_mask\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    191\u001b[0m conf_mask \u001b[38;5;241m=\u001b[39m conf_mask\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqrt()\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/loss/_region.py:247\u001b[0m, in \u001b[0;36mRegionLoss.build_targets\u001b[0;34m(self, pred_boxes, ground_truth, nB, nH, nW)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Compare prediction boxes and targets, convert targets to network output tensors \"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(ground_truth):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__build_targets_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ground_truth, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_targets_brambox(pred_boxes, ground_truth, nB, nH, nW)\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/loss/_region.py:287\u001b[0m, in \u001b[0;36mRegionLoss.__build_targets_tensor\u001b[0;34m(self, pred_boxes, ground_truth, nB, nH, nW)\u001b[0m\n\u001b[1;32m    284\u001b[0m gt[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m nH\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Set confidence mask of matching detections to 0\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m iou_gt_pred \u001b[38;5;241m=\u001b[39m \u001b[43miou_cwh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_pred_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m mask \u001b[38;5;241m=\u001b[39m (iou_gt_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou_ignore_thresh)\u001b[38;5;241m.\u001b[39many(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m conf_mask[b][mask\u001b[38;5;241m.\u001b[39mview_as(conf_mask[b])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/util/_iou.py:43\u001b[0m, in \u001b[0;36miou_cwh\u001b[0;34m(boxes1, boxes2, pairwise, type)\u001b[0m\n\u001b[1;32m     40\u001b[0m b2 \u001b[38;5;241m=\u001b[39m cwh_xyxy(boxes2, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m IoUTypes\u001b[38;5;241m.\u001b[39mIoU:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairwise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairwise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m IoUTypes\u001b[38;5;241m.\u001b[39mDIoU:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _diou(\u001b[38;5;241m*\u001b[39mb1, \u001b[38;5;241m*\u001b[39mb2, pairwise\u001b[38;5;241m=\u001b[39mpairwise)\n",
      "File \u001b[0;32m~/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/util/_iou.py:299\u001b[0m, in \u001b[0;36m_iou\u001b[0;34m(b1x1, b1y1, b1x2, b1y2, b2x1, b2y1, b2x2, b2y2, pairwise)\u001b[0m\n\u001b[1;32m    296\u001b[0m     b2y1 \u001b[38;5;241m=\u001b[39m b2y1\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    297\u001b[0m     b2y2 \u001b[38;5;241m=\u001b[39m b2y2\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 299\u001b[0m dx \u001b[38;5;241m=\u001b[39m (\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb1x2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2x2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m module\u001b[38;5;241m.\u001b[39mmaximum(b1x1, b2x1))\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    300\u001b[0m dy \u001b[38;5;241m=\u001b[39m (module\u001b[38;5;241m.\u001b[39mminimum(b1y2, b2y2) \u001b[38;5;241m-\u001b[39m module\u001b[38;5;241m.\u001b[39mmaximum(b1y1, b2y1))\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    301\u001b[0m intersections \u001b[38;5;241m=\u001b[39m dx \u001b[38;5;241m*\u001b[39m dy\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "loss_fn = ln.network.loss.RegionLoss(\n",
    "    num_classes= model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ").to(DEVICE)\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 416, 416).to(DEVICE)\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "targets = train_dataset[8][1][\"boxes\"].unsqueeze(dim=0)\n",
    "targets = targets.to(DEVICE)\n",
    "\n",
    "\n",
    "print(loss_fn(output_tensor, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/workstation/depth_estimation/codes/rgbd-yolov2/.venv/lib/python3.10/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "YoloV2 (YoloV2)                               [1, 3, 416, 416]     [1, 40, 13, 13]      --                   True\n",
       "├─FeatureExtractor (backbone)                 [1, 3, 416, 416]     [1, 1024, 13, 13]    --                   True\n",
       "│    └─Sequential (module)                    [1, 3, 416, 416]     [1, 1024, 13, 13]    --                   True\n",
       "│    │    └─Conv2dBatchAct (1_convbatch)      [1, 3, 416, 416]     [1, 32, 416, 416]    928                  True\n",
       "│    │    └─MaxPool2d (2_max)                 [1, 32, 416, 416]    [1, 32, 208, 208]    --                   --\n",
       "│    │    └─Conv2dBatchAct (3_convbatch)      [1, 32, 208, 208]    [1, 64, 208, 208]    18,560               True\n",
       "│    │    └─MaxPool2d (4_max)                 [1, 64, 208, 208]    [1, 64, 104, 104]    --                   --\n",
       "│    │    └─Conv2dBatchAct (5_convbatch)      [1, 64, 104, 104]    [1, 128, 104, 104]   73,984               True\n",
       "│    │    └─Conv2dBatchAct (6_convbatch)      [1, 128, 104, 104]   [1, 64, 104, 104]    8,320                True\n",
       "│    │    └─Conv2dBatchAct (7_convbatch)      [1, 64, 104, 104]    [1, 128, 104, 104]   73,984               True\n",
       "│    │    └─MaxPool2d (8_max)                 [1, 128, 104, 104]   [1, 128, 52, 52]     --                   --\n",
       "│    │    └─Conv2dBatchAct (9_convbatch)      [1, 128, 52, 52]     [1, 256, 52, 52]     295,424              True\n",
       "│    │    └─Conv2dBatchAct (10_convbatch)     [1, 256, 52, 52]     [1, 128, 52, 52]     33,024               True\n",
       "│    │    └─Conv2dBatchAct (11_convbatch)     [1, 128, 52, 52]     [1, 256, 52, 52]     295,424              True\n",
       "│    │    └─MaxPool2d (12_max)                [1, 256, 52, 52]     [1, 256, 26, 26]     --                   --\n",
       "│    │    └─Conv2dBatchAct (13_convbatch)     [1, 256, 26, 26]     [1, 512, 26, 26]     1,180,672            True\n",
       "│    │    └─Conv2dBatchAct (14_convbatch)     [1, 512, 26, 26]     [1, 256, 26, 26]     131,584              True\n",
       "│    │    └─Conv2dBatchAct (15_convbatch)     [1, 256, 26, 26]     [1, 512, 26, 26]     1,180,672            True\n",
       "│    │    └─Conv2dBatchAct (16_convbatch)     [1, 512, 26, 26]     [1, 256, 26, 26]     131,584              True\n",
       "│    │    └─Conv2dBatchAct (17_convbatch)     [1, 256, 26, 26]     [1, 512, 26, 26]     1,180,672            True\n",
       "│    │    └─MaxPool2d (18_max)                [1, 512, 26, 26]     [1, 512, 13, 13]     --                   --\n",
       "│    │    └─Conv2dBatchAct (19_convbatch)     [1, 512, 13, 13]     [1, 1024, 13, 13]    4,720,640            True\n",
       "│    │    └─Conv2dBatchAct (20_convbatch)     [1, 1024, 13, 13]    [1, 512, 13, 13]     525,312              True\n",
       "│    │    └─Conv2dBatchAct (21_convbatch)     [1, 512, 13, 13]     [1, 1024, 13, 13]    4,720,640            True\n",
       "│    │    └─Conv2dBatchAct (22_convbatch)     [1, 1024, 13, 13]    [1, 512, 13, 13]     525,312              True\n",
       "│    │    └─Conv2dBatchAct (23_convbatch)     [1, 512, 13, 13]     [1, 1024, 13, 13]    4,720,640            True\n",
       "├─ModuleList (neck)                           --                   --                   --                   True\n",
       "│    └─Sequential (0)                         [1, 1024, 13, 13]    [1, 1024, 13, 13]    --                   True\n",
       "│    │    └─Conv2dBatchAct (0)                [1, 1024, 13, 13]    [1, 1024, 13, 13]    9,439,232            True\n",
       "│    │    └─Conv2dBatchAct (1)                [1, 1024, 13, 13]    [1, 1024, 13, 13]    9,439,232            True\n",
       "│    └─Sequential (1)                         [1, 512, 26, 26]     [1, 256, 13, 13]     --                   True\n",
       "│    │    └─Conv2dBatchAct (0)                [1, 512, 26, 26]     [1, 64, 26, 26]      32,896               True\n",
       "│    │    └─Reorg (1)                         [1, 64, 26, 26]      [1, 256, 13, 13]     --                   --\n",
       "├─Sequential (head)                           [1, 1280, 13, 13]    [1, 40, 13, 13]      --                   True\n",
       "│    └─Conv2dBatchAct (0)                     [1, 1280, 13, 13]    [1, 1024, 13, 13]    --                   True\n",
       "│    │    └─ModuleList (layers)               --                   --                   11,798,528           True\n",
       "│    └─Conv2d (1)                             [1, 1024, 13, 13]    [1, 40, 13, 13]      41,000               True\n",
       "=============================================================================================================================\n",
       "Total params: 50,568,264\n",
       "Trainable params: 50,568,264\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 14.67\n",
       "=============================================================================================================================\n",
       "Input size (MB): 2.08\n",
       "Forward/backward pass size (MB): 258.25\n",
       "Params size (MB): 202.27\n",
       "Estimated Total Size (MB): 462.60\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = model_builder(num_classes=3)\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=model, \n",
    "        input_size=(1, 3, 416, 416), # make sure this is \"input_size\", not \"input_shape\"\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
