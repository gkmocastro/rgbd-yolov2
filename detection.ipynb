{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv2 Output Cell Shape Explanation\n",
    "\n",
    "The output cell shape of the YOLOv2 architecture refers to the structure of the final feature map produced by the network, which is used for object detection. To understand this, letâ€™s break it down step-by-step:\n",
    "\n",
    "1. **Grid Division**  \n",
    "YOLOv2 divides the input image into a grid of size S x S. Each cell in this grid is responsible for predicting bounding boxes for objects whose centers fall within that cell. For instance, if the input image size is 416 x 416, a common grid size in YOLOv2 is 13 x 13, meaning each grid cell covers 32 x 32 pixels of the original image.\n",
    "\n",
    "\n",
    "2. **Bounding Box Predictions**  \n",
    "Each grid cell predicts:\n",
    "\n",
    "A fixed number of bounding boxes (typically 5).\n",
    "For each bounding box, it predicts:\n",
    "The coordinates: (x, y, w, h) representing the center coordinates of the box relative to the cell, as well as its width and height.\n",
    "The confidence score that indicates the likelihood that the box contains an object.\n",
    "\n",
    "\n",
    "3. **Class Predictions**  \n",
    "For each bounding box, YOLOv2 also predicts the probabilities that the object belongs to one of the predefined classes. If there are C classes, then for each bounding box, there are C class scores.\n",
    "\n",
    "\n",
    "4. **Output Tensor Shape**  \n",
    "The final output of YOLOv2 has the shape: S x S x (B x (5 + C))\n",
    "Where:\n",
    "\n",
    "S x S is the grid size (e.g., 13 x 13).\n",
    "B is the number of bounding boxes predicted per grid cell (typically 5).\n",
    "5 + C refers to the 5 values for each bounding box (4 for x, y, w, h and 1 for the confidence score) plus the class predictions C.\n",
    "Example:\n",
    "For a 13 x 13 grid, 5 bounding boxes per cell, and 20 classes (like in the Pascal VOC dataset):\n",
    "\n",
    "The output shape would be 13 x 13 x (5 x (5 + 20)) = 13 x 13 x 125.\n",
    "Each cell in this final output represents predictions for multiple bounding boxes and the associated class probabilities.\n",
    "\n",
    "This grid of predictions is then post-processed using techniques like non-maximum suppression (NMS) to filter out overlapping and low-confidence boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import model_builder\n",
    "import torch\n",
    "import lightnet as ln\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkmo/workstation/mestrado/projects/rgbd-yolov2/.venv/lib/python3.12/site-packages/lightnet/network/module/_lightnet.py:293: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_file, 'cpu')\n",
      "Modules not matching, performing partial update\n"
     ]
    }
   ],
   "source": [
    "model = model_builder(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model shape\n",
    "\n",
    "def test(model):\n",
    "    X = torch.randn((2, 3, 416, 416))\n",
    "    print(model(X).shape)\n",
    "\n",
    "def test_loss(model, loss_fn):\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    X = torch.rand((1, 3, 416, 416))\n",
    "    y = torch.rand((1, 5, 5))\n",
    "    print(loss_fn)\n",
    "    with torch.inference_mode():\n",
    "        y_pred = model(X)\n",
    "\n",
    "    print(y.shape)\n",
    "    print(y_pred.shape)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 40, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "# S x S x (B x (5 + C)) -> (BATCH_SIZE, 5*(5+C), 13, 13)\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test framework loss\n",
    "\n",
    "loss_fn = ln.network.loss.RegionLoss(\n",
    "    num_classes= model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegionLoss(\n",
      "  classes=1, network_stride=32, IoU threshold=0.6, seen=0\n",
      "  coord_scale=1.0, object_scale=5.0, noobject_scale=1.0, class_scale=1.0\n",
      "  anchors=[1.3221, 1.7314] [3.1927, 4.0094] [5.0559, 8.0989] [9.4711, 4.8405] [11.236, 10.007]\n",
      ")\n",
      "{'total': tensor(120.5731), 'conf': tensor(113.4837), 'coord': tensor(7.0893), 'class': tensor(0.)}\n"
     ]
    }
   ],
   "source": [
    "model = ln.models.YoloV2(1)\n",
    "\n",
    "# Create accompanying loss (minimal required arguments for it to work with our defined Yolo network)\n",
    "loss = ln.network.loss.RegionLoss(\n",
    "    num_classes=model.num_classes,\n",
    "    anchors=model.anchors,\n",
    "    network_stride=model.stride\n",
    ")\n",
    "print(loss)\n",
    "\n",
    "# Use loss\n",
    "input_tensor = torch.rand(1, 3, 416, 416)   # batch, channel, height, width\n",
    "target_tensor = torch.rand(1, 2, 5)         # batch, num_anno, 5 (see RegionLoss docs)\n",
    "\n",
    "output_tensor = model(input_tensor)\n",
    "loss_value = loss(output_tensor, target_tensor)\n",
    "loss_value.backward()\n",
    "\n",
    "# Print loss\n",
    "print(loss.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path(\"data\")\n",
    "labels_dir = root_dir / \"labels\"\n",
    "img_dir = root_dir / \"data_object_image_2/training/image_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7481"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list((labels_dir).glob(\"*.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/labels')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class YoloDarknetDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, classes, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str or Path): Path to the directory containing images.\n",
    "            labels_dir (str or Path): Path to the directory containing labels.\n",
    "            classes (list): List of class names.\n",
    "            transform (callable, optional): Transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.labels_dir = Path(labels_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "\n",
    "        # Gather all image files in the directory\n",
    "        self.image_files = sorted([p for p in self.images_dir.glob('*') if p.suffix in ['.jpg', '.jpeg', '.png']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.image_files[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Load the corresponding label file\n",
    "        label_path = self.labels_dir / f\"{img_path.stem}.txt\"\n",
    "        boxes, labels = self._load_labels(label_path)\n",
    "\n",
    "        # Convert boxes and labels to tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return img, {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "    def _load_labels(self, label_path):\n",
    "        \"\"\"\n",
    "        Load labels from a Darknet format .txt file without converting to pixel coordinates.\n",
    "        \n",
    "        Args:\n",
    "            label_path (Path): Path to the .txt file.\n",
    "        \n",
    "        Returns:\n",
    "            boxes (list of lists): Bounding boxes in normalized coordinates [x_center, y_center, width, height].\n",
    "            labels (list): Class labels.\n",
    "        \"\"\"\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    labels.append(int(class_id))\n",
    "                    boxes.append([x_center, y_center, width, height])\n",
    "\n",
    "        return boxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kitti_dataset = YoloDarknetDataset(images_dir=img_dir, labels_dir=labels_dir, classes=[\"Cyclist\", \"Pedestrian\", \"car\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Kitti_dataset[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1242x375>,\n",
       " {'boxes': tensor([[0.1620, 0.7552, 0.3239, 0.4843],\n",
       "          [0.3862, 0.7346, 0.2332, 0.5149],\n",
       "          [0.8769, 0.7619, 0.2445, 0.4710],\n",
       "          [0.5308, 0.5831, 0.0993, 0.2266],\n",
       "          [0.6173, 0.5030, 0.0411, 0.1056],\n",
       "          [0.7411, 0.5580, 0.0579, 0.1650]]),\n",
       "  'labels': tensor([2, 2, 2, 2, 2, 2])})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7481"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Kitti_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
